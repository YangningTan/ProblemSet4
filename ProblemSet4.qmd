---
title: "ProblemSet4"
author: "Yangning Tan"
format: html
editor: visual
---

## Problem 1 - Tidyverse

```{r}
# install package
library(nycflights13)
library(tidyverse)
```

a.  Generate a table (which can just be a nicely printed tibble) reporting the mean and median departure delay per airport. Generate a second table (which again can be a nicely printed tibble) reporting the mean and median arrival delay per airport. Exclude any destination with under 10 flights. Do this exclusion through code, not manually.

    Additionally,

    -   Order both tables in descending mean delay.

    -   Both tables should use the airport *names* not the airport *codes*.

    -   Both tables should print all rows.

    ```{r}
    # mean and median of departure delay
    tb_flights <- nycflights13::flights

    tb_flights %>% 
      left_join(nycflights13::airports, by = c("origin" = "faa")) %>% 
      select(name, dep_delay) %>% 
      group_by(name) %>% 
      summarise(dep_delay_mean = mean(dep_delay, na.rm = TRUE),
                dep_delay_median = median(dep_delay, na.rm = TRUE)) %>% 
      arrange(desc(dep_delay_mean))
    ```

    ```{r}
    # mean and median of arrival delay
    tb_flights %>%
      #left_join(nycflights13::airports, by = c("dest" = "faa")) %>%
      group_by(dest) %>%
      filter(n() >= 10) %>%
      ungroup() %>% 
      select(dest, arr_delay) %>%
      group_by(dest) %>%
      summarise(arr_delay_mean = mean(arr_delay, na.rm = TRUE),
                arr_delay_median = median(arr_delay, na.rm = TRUE)) %>% 
      left_join(nycflights13::airports, by = c("dest" = "faa")) %>%
      select(name, arr_delay_mean, arr_delay_median) %>% 
      arrange(desc(arr_delay_mean))
    ```

b.  How many flights did the aircraft model with the fastest average speed take? Produce a tibble with 1 row, and entires for the model, average speed (in MPH) and number of flights.

    We first create a tibble of the planes.

    ```{r}
    tb_planes <- nycflights13::planes
    ```

    We need to get the average speed of each aircraft. To do this, we join the tibble of flights and planes.

    ```{r}
    tb_flights %>% 
      left_join(tb_planes, by = c("tailnum" = "tailnum")) %>% 
      select(model, distance, air_time) %>% 
      group_by(model) %>% 
      summarise(total_distance = sum(distance, na.rm = TRUE),
                total_time = sum(air_time, na.rm = TRUE) / 60,
                average_speed = total_distance / total_time,
                number_of_flights = n()) %>% 
      ungroup() -> average_speed
    ```

    Then, we can find out the model of plane with the fastest average speed.

    ```{r}
    average_speed %>% 
      filter(average_speed == max(average_speed)) %>% 
      select(model, average_speed, number_of_flights) -> fastest_model
    fastest_model
    ```

## Problem 2

Load the Chicago NNMAPS data we used in the visualization lectures. Write a function `get_temp()` that allows a user to request the average temperature for a given month.

We first import the data.

```{r}
nnmaps <- read.csv("/Users/tyn/Documents/R/chicago-nmmaps.csv")
```

Then we write the function.

```{r}
get_temp <- function(month_input, year_input, data, celsius = FALSE, average_fn = mean){
  # check the validity of input year
  if (year_input < 1997 | year_input > 2000) {
    stop("Invalid input: Year should be between 1997 and 2000")
  }
  
  # convert all kinds of input of month into numeric form
  convert_month_to_numeric <- function(month_input) {
  month_mapping <- c("Jan" = 1, "Feb" = 2, "Mar" = 3, "Apr" = 4, "May" = 5, "Jun" = 6, "Jul" = 7, "Aug" = 8, "Sep" = 9, "Oct" = 10, "Nov" = 11, "Dec" = 12)
  
  if (is.numeric(month_input)) {
    # Check if the input is a numeric value between 1 and 12
    if (month_input >= 1 && month_input <= 12) {
      return(month_input)
    } else {
      stop("Invalid input: Numeric month should be between 1 and 12.")
    }
  } else if (is.character(month_input)) {
    # Check if the input is a valid month name or abbreviation
    formatted_input <- substr(month_input, 1, 3)
    if (formatted_input %in% names(month_mapping)) {
      return(month_mapping[formatted_input])
    } else {
      stop("Invalid input: Not a recognized month name or abbreviation.")
    }
  } else {
    stop("Invalid input: Input should be a numeric value, a valid month name, or a valid month abbreviation.")
    }
  }
  month_input <- convert_month_to_numeric(month_input)
  
  # filter the data
  filtered_data <- data %>%
    select(temp, year, month_numeric) %>% 
    group_by(year, month_numeric) %>% 
    summarize(mean_temp = average_fn(temp)) %>% 
    ungroup() %>% 
    filter(year == year_input, month_numeric == month_input)
  
  average_temp <- filtered_data$mean_temp
  
  # transfer into celsius if necessary
  if (celsius) {
    average_temp <- (average_temp - 32) * 5/9
  }
    
  return(average_temp)
}
```

Finally, we check whether the code works.

```{r}
# valid input
get_temp("Apr", 1999, data = nnmaps)
get_temp("Apr", 1999, data = nnmaps, celsius = TRUE)
get_temp(10, 1998, data = nnmaps, average_fn = median)
get_temp("November", 1999, data =nnmaps, celsius = TRUE,
         average_fn = function(x) {
           x %>% sort -> x
           x[2:(length(x) - 1)] %>% mean %>% return
         })
```

If the input is valid, we can get a result. When the input is invalid, it can produce a reasonable error message.

## Problem 3

We first import the data set.

```         
/* data libraries for reading/writing data: -------------------------------- */
%let in_path = ~/sasuser/input_data;
%let out_path = ~/sasuser/output_data; 
libname in_lib "&in_path."; 
libname out_lib "&out_path.";

/* import data */
data recs; 
 set in_lib.recs2020_public_v5; 
```

a.  What state has the highest percentage of records? What percentage of all records correspond to Michigan? 

    ```         
    data recs_filtered;
       set recs;
       keep state_name NWEIGHT;
    run;

    proc summary data=recs_filtered nway;
      class state_name;
      var NWEIGHT;
      output out=state_nweight sum(NWEIGHT)=NWEIGHT;
    run;

    proc sql;
      select sum(NWEIGHT) into :total_NWEIGHT
      from state_nweight;
    quit;

    data state_nweight;
      set state_nweight;
      percentage = (NWEIGHT / &total_NWEIGHT) * 100;
    run;
    ```

    The state with the highest percentage of record is California.

    The percentage corresponds to Michigan is 3.1724415122.

b.  Generate a histogram of the total electricity cost in dollars, amongst those with a strictly positive cost.

    ```         
    data recs_electric;
     set recs(keep=DOLLAREL);
     where DOLLAREL > 0;
    run;

    proc univariate data=recs_electric;
      histogram DOLLAREL / midpoints=5 to 50 by 5;
    run; 
    ```

c.  Generate a histogram of the log of the total electricity cost.

    ```         
    data recs_electric_log;
      set recs_electric;
      log_DOLLAREL = log(DOLLAREL);
    run;

    proc univariate data=recs_electric_log;
      histogram log_DOLLAREL / midpoints=2 to 5 by 0.2;
    run;
    ```

d.  Fit a linear regression model predicting the log of the total electricity cost based upon the number of rooms in the house and whether or not the house has a garage.

    ```         
    data recs_regression;
     set recs;
     where DOLLAREL > 0;
     where PRKGPLC1 > -1;
     keep DOEID DOLLAREL TOTROOMS PRKGPLC1 NWEIGHT;
     /*log_DOLLAREL = log(DOLLAREL);*/
    run;

    data recs_regression_log;
     set recs_regression;
     log_DOLLAREL = log(DOLLAREL);
    run;

    proc reg data=recs_regression_log outest=reg_model;
      model log_DOLLAREL = TOTROOMS PRKGPLC1;
      weight NWEIGHT;
    run;
    ```

e.  Use that model to generate predicted values and create a scatterplot of predicted total electricity cost vs actual total electricity cost.

    ```         
    proc reg data=recs_regression_log outest=reg_model;
      model log_DOLLAREL = TOTROOMS PRKGPLC1;
      weight NWEIGHT;
      output out=predicted_values_log predicted=log_DOLLAREL_pred;
    run;

    data recs_predicted_log;
      merge recs_regression_log predicted_values_log;
      by DOEID;
    run;

    data recs_predicted;
      set recs_predicted_log;
      DOLLAREL_pred = exp(log_DOLLAREL_pred);
    run;

    proc sgplot data=recs_predicted;
      scatter x=DOLLAREL y=DOLLAREL_pred;
      xaxis label="actual total electricity cost";
      yaxis label="predicted total electricity cost";
    run;
    ```

## Problem 4

a.  Take a look at the Codebook. For very minor extra credit, how was the Codebook generated?

    It indicates the origin of the data set. In fact, some of the data is not included in this public version. Also, it lists all the variables. For each variable, it further provides a summary.

b.  Import the data into SAS and use `proc sql` to select only the variables you'll need for your analysis, as well as subsetting the data if needed.

    ```         
    data fin;
     set in_lib.public2022;
    run;

    proc sql;
      create table fin_filtered as
      select CaseID, B3, ND2, B7_b, GH1, ppeducat, race_5cat ,weight_pop
      from fin;
    quit;
    ```

c.  Get the data out of SAS and into Stata. 

    We export a csv file from SAS first.

    ```         
    proc export data=fin_filtered
        outfile = "&out_path./fin.csv"
        dbms=csv;
    run;
    ```

d.  Demonstrate that you've successfully extracted the appropriate data by showing the number of observations and variables.

    ```         
    import delimited "/Users/tyn/Documents/Stata/fin.csv"
    describe

    Contains data
     Observations:        11,667                  
        Variables:             8                  
    -----------------------------------------------------------------------------------------
    Variable      Storage   Display    Value
        name         type    format    label      Variable label
    -----------------------------------------------------------------------------------------
    caseid          int     %8.0g                 CaseID
    b3              byte    %8.0g                 B3
    nd2             byte    %8.0g                 ND2
    b7_b            byte    %8.0g                 B7_b
    gh1             byte    %8.0g                 GH1
    ppeducat        byte    %8.0g                 
    race_5cat       byte    %8.0g                 
    weight_pop      float   %9.0g                 
    -----------------------------------------------------------------------------------------
    ```

e.  The response variable is a Likert scale; convert it to a binary of worse off versus same/better.

    ```         
    .gen fin_sit = .
    .replace fin_sit = 0 if b3 <= 2
    .replace fin_sit = 1 if b3 >= 3
    ```

f.  Carry out a logisitic regression model accounting for the complex survey design. Be sure to treat variables you think should be categorical appropriately. From these results, provide an answer to the researchers question of interest.

    ```         
    .svyset caseid [pw=weight_pop]
    .svy: logit fin_sit i.nd2 i.b7_b i.gh1 i.ppeducat i.race_5cat

    Survey: Logistic regression

    Number of strata =      1                        Number of obs   =      11,667
    Number of PSUs   = 11,667                        Population size = 255,114,223
                                                     Design df       =      11,666
                                                     F(17, 11650)    =       56.70
                                                     Prob > F        =      0.0000

    ------------------------------------------------------------------------------
                 |             Linearized
         fin_sit | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
    -------------+----------------------------------------------------------------
             nd2 |
              2  |   .0816722   .0925755     0.88   0.378    -.0997913    .2631356
              3  |   .0618535   .0854686     0.72   0.469    -.1056792    .2293863
              4  |   .2533888   .2045978     1.24   0.216    -.1476572    .6544347
              5  |    .229354   .1672799     1.37   0.170    -.0985426    .5572505
                 |
            b7_b |
              2  |   1.110649   .0488662    22.73   0.000     1.014863    1.206435
              3  |   1.806251   .0796863    22.67   0.000     1.650052    1.962449
              4  |   2.485125   .3463415     7.18   0.000     1.806238    3.164013
                 |
             gh1 |
              2  |  -.0702921    .056382    -1.25   0.213    -.1808102     .040226
              3  |   .0190607   .0587346     0.32   0.746    -.0960689    .1341904
              4  |   .3465325   .0994184     3.49   0.000     .1516557    .5414092
                 |
        ppeducat |
              2  |   .0767668   .1036364     0.74   0.459    -.1263778    .2799115
              3  |   .1075004   .1008067     1.07   0.286    -.0900975    .3050983
              4  |   .2288346    .099574     2.30   0.022     .0336528    .4240164
                 |
       race_5cat |
              2  |   .7060141   .0810818     8.71   0.000     .5470803     .864948
              3  |   .1635498   .0711263     2.30   0.021     .0241303    .3029693
              4  |   .4567994   .1259942     3.63   0.000     .2098298    .7037691
              5  |  -.0210142   .1659436    -0.13   0.899    -.3462915    .3042631
                 |
           _cons |  -.4852955   .1301287    -3.73   0.000    -.7403696   -.2302214
    ------------------------------------------------------------------------------
    ```

    We can see from the result of the logistic regression that when other variables are controlled, the p-value of variable "nd2" is greater than 0.05. Therefore, we get the conclusion that the coefficient is positive. This implies that **the respondent's family is better off, the same, or worse off finanicially compared to 12 month's ago** CAN be predicted by **thinking that the chance of experiencing a natural disaster or severe weather event will be higher, lower or about the same in 5 years**.

g.  Get the data out of Stata and into R.

    ```         
    .export delimited using "fin_2.csv", replace
    ```

h.  Obtain the pseudo-$R^2$ value for the logistic model fit above and report it.

    We first import the data.

    ```{r}
    dat <- read.csv("/Users/tyn/Documents/R/fin_2.csv")
    ```

    Now, we obtain the pseudo-$R^2$.

    ```{r}
    library(survey)
    # set up the complex survey design
    svy_design <- svydesign(id = ~ caseid, weight = ~ weight_pop, data = dat)
    # fit the logistic regression model
    survey_logit_model <- svyglm(fin_sit ~ as.factor(nd2) + as.factor(b7_b) + as.factor(gh1) + as.factor(ppeducat) + as.factor(race_5cat), design = svy_design, family = quasibinomial)
    # compute the pseudo R squared
    pseudo_r2 <- 1 - survey_logit_model$deviance / survey_logit_model$null.deviance
    pseudo_r2
    ```

    The pseudo-$R^2$ is approximately 0.08967482.
